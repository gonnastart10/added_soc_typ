{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи: нам предлагается файл с данными о покупках различных паков в игре W.. \n",
    "# Необходимо изучить ассоциативные правила, которые мы можем построить на основании предложенных данных.\n",
    "# Наибольший интерес вызывают акционные паки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По ссылке, приведенной ниже, можно посомотреть диаграмму Ганта. Она позволяет наглядно показать акционные/неакционные паки.\n",
    "\n",
    "https://public.tableau.com/profile/roman2610#!/vizhome/Book_207/Dashboard2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всевозможные паки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего паков:  214\n"
     ]
    }
   ],
   "source": [
    "#social_netork='fb'\n",
    "pcks=[]\n",
    "with open('trans.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='|', quotechar='|')\n",
    "    pcks=[]\n",
    "    spamreader.next()\n",
    "    for row in spamreader:\n",
    "        if (row[5] not in pcks):\n",
    "            pcks.append(row[5])\n",
    "    print \"Всего паков: \",len(pcks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Априори алгоритм: (параллельно подготовка словаря для tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "from optparse import OptionParser\n",
    "\n",
    "\n",
    "def subsets(arr):\n",
    "    \"\"\" Returns non empty subsets of arr\"\"\"\n",
    "    return chain(*[combinations(arr, i + 1) for i, a in enumerate(arr)])\n",
    "\n",
    "\n",
    "def returnItemsWithMinSupport(itemSet, transactionList, minSupport, freqSet):\n",
    "        \"\"\"calculates the support for items in the itemSet and returns a subset\n",
    "       of the itemSet each of whose elements satisfies the minimum support\"\"\"\n",
    "        _itemSet = set()\n",
    "        localSet = defaultdict(int)\n",
    "\n",
    "        for item in itemSet:\n",
    "                for transaction in transactionList:\n",
    "                        if item.issubset(transaction):\n",
    "                                freqSet[item] += 1\n",
    "                                localSet[item] += 1\n",
    "\n",
    "        for item, count in localSet.items():\n",
    "                support = float(count)/len(transactionList)\n",
    "\n",
    "                if support >= minSupport:\n",
    "                        _itemSet.add(item)\n",
    "\n",
    "        return _itemSet\n",
    "\n",
    "\n",
    "def joinSet(itemSet, length):\n",
    "        \"\"\"Join a set with itself and returns the n-element itemsets\"\"\"\n",
    "        return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])\n",
    "\n",
    "\n",
    "def getItemSetTransactionList(data_iterator):\n",
    "    transactionList = list()\n",
    "    itemSet = set()\n",
    "    for record in data_iterator:\n",
    "        transaction = frozenset(record)\n",
    "        transactionList.append(transaction)\n",
    "        for item in transaction:\n",
    "            itemSet.add(frozenset([item]))              # Generate 1-itemSets\n",
    "    return itemSet, transactionList\n",
    "\n",
    "\n",
    "def runApriori(data_iter, minSupport, minConfidence):\n",
    "    \"\"\"\n",
    "    run the apriori algorithm. data_iter is a record iterator\n",
    "    Return both:\n",
    "     - items (tuple, support)\n",
    "     - rules ((pretuple, posttuple), confidence)\n",
    "    \"\"\"\n",
    "    itemSet, transactionList = getItemSetTransactionList(data_iter)\n",
    "\n",
    "    freqSet = defaultdict(int)\n",
    "    largeSet = dict()\n",
    "    # Global dictionary which stores (key=n-itemSets,value=support)\n",
    "    # which satisfy minSupport\n",
    "\n",
    "    assocRules = dict()\n",
    "    # Dictionary which stores Association Rules\n",
    "\n",
    "    oneCSet = returnItemsWithMinSupport(itemSet,\n",
    "                                        transactionList,\n",
    "                                        minSupport,\n",
    "                                        freqSet)\n",
    "\n",
    "    currentLSet = oneCSet\n",
    "    k = 2\n",
    "    while(currentLSet != set([])):\n",
    "        largeSet[k-1] = currentLSet\n",
    "        currentLSet = joinSet(currentLSet, k)\n",
    "        currentCSet = returnItemsWithMinSupport(currentLSet,\n",
    "                                                transactionList,\n",
    "                                                minSupport,\n",
    "                                                freqSet)\n",
    "        currentLSet = currentCSet\n",
    "        k = k + 1\n",
    "\n",
    "    def getSupport(item):\n",
    "            \"\"\"local function which Returns the support of an item\"\"\"\n",
    "            return float(freqSet[item])/len(transactionList)\n",
    "\n",
    "    toRetItems = []\n",
    "    for key, value in largeSet.items():\n",
    "        toRetItems.extend([(tuple(item), getSupport(item))\n",
    "                           for item in value])\n",
    "\n",
    "    toRetRules = []\n",
    "    for key, value in largeSet.items()[1:]:\n",
    "        for item in value:\n",
    "            _subsets = map(frozenset, [x for x in subsets(item)])\n",
    "            for element in _subsets:\n",
    "                remain = item.difference(element)\n",
    "                if len(remain) > 0:\n",
    "                    confidence_lift = []\n",
    "                    item_A=getSupport(element)\n",
    "                    item_B=getSupport(remain)\n",
    "                    confidence_lift.append(getSupport(item)/item_A)#conf - 0\n",
    "                    if confidence_lift[0] >= minConfidence:\n",
    "                        confidence_lift.append(getSupport(item)/(item_A*item_B))#lift - 1\n",
    "                        if  item_A>item_B:\n",
    "                            confidence_lift.append(item_B)#min(minsup) - 2\n",
    "                        else:\n",
    "                            confidence_lift.append(item_A)\n",
    "                        toRetRules.append(((tuple(element), tuple(remain)),\n",
    "                                           confidence_lift))\n",
    "    return toRetItems, toRetRules\n",
    "\n",
    "\n",
    "\n",
    "def dataFromFile(fname):\n",
    "        \"\"\"Function which reads from the file and yields a generator\"\"\"\n",
    "        file_iter = open(fname, 'rU')\n",
    "        for line in file_iter:\n",
    "                line = line.strip().rstrip(',')                         # Remove trailing comma\n",
    "                record = frozenset(line.split(','))\n",
    "                yield record\n",
    "                \n",
    "def printResults(items, rules, rs,it, idc, t_d,start_date,end_date, len_ids): \n",
    "                                        #принимает: итемы для минсапа, правила с конф, файл для записи результата,\n",
    "                                        #название текущего пака, номер айдишника для записи в словарь данных по пакам (tableau),\n",
    "                                        #словарь, дата начала и конца, кол-во транзакций в периоде\n",
    "    \"\"\"prints the generated itemsets sorted by support and the confidence rules sorted by confidence\"\"\"\n",
    "    if len(rules)!=0:\n",
    "        rs.write(\"SUP------------SUP------------SUP:\\n\")\n",
    "        rs.write(\"For time boundaries on: \"+it+\"\\n\") #показывает периоды какого пака исследуются\n",
    "        for item, support in sorted(items, key=lambda (item, support): support):\n",
    "            rs.write(\"item: %s , %.3f \\n\" % (str(item), support))\n",
    "            #print \"item: %s , %.3f\" % (str(item), support)\n",
    "        rs.write(\"\\nRULES------------RULES------------RULES:\\n\")\n",
    "        for rule, confidence in sorted(rules, key=lambda (rule, confidence): confidence):\n",
    "            pre, post = rule\n",
    "            idc=idc+1\n",
    "            t_d[idc]=[]\n",
    "            st_st=\"%s ==> %s\"%(str(pre), str(post))\n",
    "            t_d[idc].append(st_st)#rule \n",
    "            t_d[idc].append(confidence[0])#conf\n",
    "            t_d[idc].append(confidence[1])#lift\n",
    "            t_d[idc].append(confidence[2])#min minsup\n",
    "            if \"fb_\" in str(pre)[:7]:\n",
    "                if \"fb_\" in str(post)[:7]:\n",
    "                    t_d[idc].append(\"fb\") #social_network\n",
    "                else:\n",
    "                    t_d[idc].append(\"cross\")\n",
    "            elif \"vk_\" in str(pre)[:7]:\n",
    "                if \"vk_\" in str(post)[:7]:\n",
    "                    t_d[idc].append(\"vk\") #social_network\n",
    "                else:\n",
    "                    t_d[idc].append(\"cross\")\n",
    "            elif \"ok_\" in str(pre)[:7]:\n",
    "                if \"ok_\" in str(post)[:7]:\n",
    "                    t_d[idc].append(\"ok\") #social_network\n",
    "                else:\n",
    "                    t_d[idc].append(\"cross\")\n",
    "            elif \"mm_\" in str(pre)[:7]:\n",
    "                if \"mm_\" in str(post)[:7]:\n",
    "                    t_d[idc].append(\"mm\") #social_network\n",
    "                else:\n",
    "                    t_d[idc].append(\"cross\")\n",
    "            else:\n",
    "                t_d[idc].append(\" \")\n",
    "            date_for_period=str(date.fromtimestamp(start_date))+\" to \"+str(date.fromtimestamp(end_date))\n",
    "            t_d[idc].append(date_for_period)#date\n",
    "            t_d[idc].append(len_ids)# number of transactions in period\n",
    "            \n",
    "            l_s=str(pre)\n",
    "            r_s=str(post)\n",
    "            access_act = False\n",
    "            l=l_s.strip(\"(\").strip(\")\").strip(\",\").split(\",\")\n",
    "            for i in range(0,len(l)):\n",
    "                l[i]=l[i].strip(\"'\")\n",
    "                if l[i] in action_pack:\n",
    "                    access_act=True\n",
    "            r=r_s.strip(\"(\").strip(\")\").strip(\",\").split(\",\")\n",
    "            for i in range(0,len(r)):\n",
    "                r[i]=r[i].strip(\"'\")\n",
    "                if r[i] in action_pack:\n",
    "                    access_act=True\n",
    "            if access_act==True:\n",
    "                t_d[idc].append(\"y\")# if rule consist action pack\n",
    "            else:\n",
    "                t_d[idc].append(\"n\")\n",
    "            rs.write(\"Rule: %s ==> %s , confidence: %.3f, lift: %.3f, min(minsup) %.3f\\n\" % (str(pre), str(post), confidence[0], confidence[1], confidence[2]))\n",
    "        rs.write(\"\\n-----------------------------------------\\n\")\n",
    "        rs.write(\"\\n\")\n",
    "    return idc, t_d\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    optparser = OptionParser()\n",
    "    optparser.add_option('-f', '--inputFile',\n",
    "                         dest='input',\n",
    "                         help='filename containing csv',\n",
    "                         default=None)\n",
    "    optparser.add_option('-s', '--minSupport',\n",
    "                         dest='minS',\n",
    "                         help='minimum support value',\n",
    "                         default=0.15,\n",
    "                         type='float')\n",
    "    optparser.add_option('-c', '--minConfidence',\n",
    "                         dest='minC',\n",
    "                         help='minimum confidence value',\n",
    "                         default=0.6,\n",
    "                         type='float')\n",
    "\n",
    "    (options, args) = optparser.parse_args()\n",
    "\n",
    "\n",
    "    inFile = dataFromFile(\"without_bound.csv\")\n",
    "\n",
    "    minSupport = options.minS\n",
    "    minConfidence = options.minC\n",
    "\n",
    "    items, rules = runApriori(inFile, minSupport, minConfidence)\n",
    "\n",
    "    #printResults(items, rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем словарь, содержащий все акционные паки со всеми периодами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "        action_pack={}\n",
    "        for it in pcks:\n",
    "            times=[]\n",
    "            # 1) составляем временные промежутки для паков\n",
    "\n",
    "            with open('trans.csv', 'rb') as csvfile:\n",
    "                spamreader = csv.reader(csvfile, delimiter='|', quotechar='|')\n",
    "                spamreader.next()\n",
    "                for row in spamreader:\n",
    "                    if row[5]==it:\n",
    "                        times.append(int(row[2]))\n",
    "\n",
    "            difference=172800+43200 # = 2.5 days in seconds\n",
    "            # сравниваем даты по timestamp\n",
    "            # если покупки наблюдались в окрестности 2х дней, то\n",
    "            # будем считать, что в эти дни акция действовала\n",
    "            times.sort()\n",
    "\n",
    "            start_time=[]\n",
    "            end_time=[]\n",
    "\n",
    "            beg=times[0]\n",
    "\n",
    "            start_time.append(beg)\n",
    "            for item in times:\n",
    "                if beg+difference>=item:\n",
    "                    beg=item\n",
    "                else:\n",
    "                    start_time.append(item)\n",
    "                    end_time.append(beg)\n",
    "                    beg=item\n",
    "            end_time.append(beg)\n",
    "\n",
    "            # 2) проверяем временные промежутки; если все промежутки лежат в рамках 2.5 недель,\n",
    "            # то считаем пак акционным и проходимся априори алгоритмом по выявленным промежуткам\n",
    "\n",
    "            week_time=604800 # week time\n",
    "            access=True\n",
    "            for i in range(0,len(start_time)):\n",
    "                if end_time[i]-start_time[i] > week_time*2.5: # предполагаем, что акции длятся не более 2х с половиной недель\n",
    "                    access=False\n",
    "            if access==True:\n",
    "                action_pack[it]=[]\n",
    "                action_pack[it].append(start_time)\n",
    "                action_pack[it].append(end_time)\n",
    "                \n",
    "#action_pack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Априори для нашего файла. Соответсвенно возможны пересечения результатов (некоторые акции запускаются в одно и то же время)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    from datetime import datetime\n",
    "    from datetime import date\n",
    "    def write_res_to_file_name(file_name, mnsp, mncf, t_d):\n",
    "        id_count=0\n",
    "        now = datetime.now()\n",
    "        minSupport=mnsp\n",
    "        minConfidence=mncf\n",
    "    \n",
    "        count=0\n",
    "        with open(file_name, 'w') as rs:\n",
    "            for it,time_st_end in action_pack.items():\n",
    "                start_time=time_st_end[0]\n",
    "                end_time=time_st_end[1]\n",
    "                for bound in range(0,len(start_time)):\n",
    "                    ids={}\n",
    "                    # 3) если пак акционный, то забиваем данные для составления ассоциативных правил\n",
    "\n",
    "                    with open('trans.csv', 'rb') as csvfile:\n",
    "                        spam = csv.reader(csvfile, delimiter='|', quotechar='|')\n",
    "                        spam.next()\n",
    "                        for row in spam:\n",
    "                            if (int(row[2])>=start_time[bound] and int(row[2])<=end_time[bound]):\n",
    "                                if ids.has_key(int(row[0]))==True:\n",
    "                                    val=ids.get(int(row[0]))\n",
    "                                    if row[5] not in val:\n",
    "                                        val.append(row[5])\n",
    "                                else:\n",
    "                                    ids[int(row[0])]=[]# каждый id принимает в значение свои транзакции\n",
    "                                    ids[int(row[0])].append(row[5])\n",
    "\n",
    "                    with open('file_for_transactions.csv', 'w') as csf:\n",
    "                        for key, value in ids.items():\n",
    "                            st=\"\"\n",
    "                            for i in range(0,len(value)):\n",
    "                                st=st+value[i]+','\n",
    "                            csf.write(st+'\\n')\n",
    "                    #now1 = datetime.now()\n",
    "                    #print \"файлы: \",\"  \",(now1-now)\n",
    "\n",
    "                    inFile = dataFromFile(\"file_for_transactions.csv\")\n",
    "                    items, rules = runApriori(inFile, minSupport, minConfidence)    \n",
    "                    id_count, t_d=printResults(items, rules, rs,it, id_count, t_d, start_time[bound],end_time[bound],len(ids))\n",
    "                    #print len(ids),\"-\",\n",
    "                    \n",
    "\n",
    "\n",
    "        # 4) время работы алгоритма\n",
    "        now1 = datetime.now()\n",
    "        print (now1-now)\n",
    "        return t_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем априори на наших данные. Подаем название файла, в который записываются результаты, minSupport, minConfidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:29.343313\n"
     ]
    }
   ],
   "source": [
    "tabl_dict={}\n",
    "tabl_dic=write_res_to_file_name(\"res_l1_check_2.txt\", 0.05, 0.6,tabl_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем файл для tableau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('good_thing_to_do.csv', 'w') as csvfile_1:\n",
    "    fieldnames=[\"ID\",\"Rule\",\"Confidence\",\"Lift\",\"Support\",\"Social_network\",\"Date\",\"Number_of_transactions_in_period\",\"Consist_action_pack\"]\n",
    "    writer = csv.DictWriter(csvfile_1, fieldnames=fieldnames,delimiter='|')\n",
    "    writer.writeheader()\n",
    "    for key,value in tabl_dict.items():\n",
    "        #print value[4],\n",
    "        writer.writerow({fieldnames[0]: key, fieldnames[1]: value[0], fieldnames[2]: value[1], fieldnames[3]: value[2],fieldnames[4]: value[3],fieldnames[5]: value[4],fieldnames[6]: value[5],fieldnames[7]: value[6],fieldnames[8]: value[7]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты можно посомтреть по ссылке:\n",
    "https://public.tableau.com/profile/publish/Rule_Inf_about_it/Dashboard3#!/publish-confirm\n",
    "\n",
    "Основные моменты: чем больше транзакций, тем лучше (на наших данных, при кол-ве транзакций >100, в большинстве это правила внутри соц. сети - одноклассники), нужно исключить лифт = 1 (иначе данные независимы).\n",
    "\n",
    "Также можно проследить в каких правилах учавствуют акционные паки. Если выделить только эти правила, то можно заметить, что во всех периодах наблюдается слишком мало транзакций. Такой результат можно интерпретировать либо как то, что в большинстве акции не влияют на результаты покупок (если, к примеру, акции должны приводить к покупкам данных паков пользователей с различных соц.сетей), либо, как то, что необходимо рассматривать правила по некоторым дополнительным признакам (как, к примеру, внутри одной соц. сети), т.к. иначе акционные паки не проходят порог минимального саппорта, который в нашем случае итак слишком маленький."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теперь хотим посомтреть ассоцитивные правила, включая социальные данные. Пока что будем включать только страну.\n",
    "\n",
    "Обработка файла:\n",
    "Csv библиотека не воспринимает \"||\", поэтому добавляем пробелы между каждыми \"||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=1\n",
    "with open('w_pinfo.csv', 'w') as csv:\n",
    "    with open('wlitems_pinfo.csv', 'rb') as csvfile:\n",
    "        for line in csvfile:\n",
    "                temp=\"\"\n",
    "                if \"||\" in line:\n",
    "                    for i in range(0,len(line)):\n",
    "                        temp=temp+line[i]\n",
    "                        if i!=len(line)-1 and line[i]==\"|\":\n",
    "                            if line[i+1]==\"|\":\n",
    "                                temp=temp+\" \"\n",
    "                    csv.write(temp)\n",
    "                else:\n",
    "                    csv.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем словарь айди-соц данные. Пока что тут лежит только страна. Ошибка выдается на нечисловой айдишник."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erros has been found\n",
      "0:00:05.777879\n",
      "0:00:01.784865\n"
     ]
    }
   ],
   "source": [
    "import sys, errno\n",
    "\n",
    "now = datetime.now()\n",
    "# 4 - country\n",
    "id_soc_data={}\n",
    "with open('w_pinfo.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='|', quotechar='|')\n",
    "    spamreader.next()\n",
    "    for row in spamreader:\n",
    "        try:\n",
    "            if id_soc_data.has_key(int(row[0]))==False:\n",
    "                id_soc_data[int(row[0])]=[]\n",
    "                id_soc_data[int(row[0])].append(row[5])\n",
    "        except ValueError as e:\n",
    "            print(\"Error has been found\") #вылез странный айдишник 2.392...Е, пока что кидаю в еррор\n",
    "            now1 = datetime.now()\n",
    "            print(now1-now)\n",
    "            now = datetime.now()\n",
    "now1 = datetime.now()\n",
    "print (now1-now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Априори на наших данных+страна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printResults(items, rules, rs,it):\n",
    "    \"\"\"prints the generated itemsets sorted by support and the confidence rules sorted by confidence\"\"\"\n",
    "    if len(rules)!=0:\n",
    "        rs.write(\"SUP------------SUP------------SUP:\\n\")\n",
    "        rs.write(\"For time boundaries on: \"+it+\"\\n\") #показывает периоды какого пака исследуются\n",
    "        for item, support in sorted(items, key=lambda (item, support): support):\n",
    "            rs.write(\"item: %s , %.3f \\n\" % (str(item), support))\n",
    "            #print \"item: %s , %.3f\" % (str(item), support)\n",
    "        rs.write(\"\\nRULES------------RULES------------RULES:\\n\")\n",
    "        for rule, confidence in sorted(rules, key=lambda (rule, confidence): confidence):\n",
    "            pre, post = rule\n",
    "            rs.write(\"Rule: %s ==> %s , confidence: %.3f, lift: %.3f\\n\" % (str(pre), str(post), confidence[0], confidence[1]))\n",
    "        rs.write(\"\\n-----------------------------------------\\n\")\n",
    "        rs.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def write_res_to_file_name_with_additional_data(file_name, mnsp, mncf):\n",
    "    now = datetime.now()\n",
    "    minSupport=mnsp\n",
    "    minConfidence=mncf\n",
    "    \n",
    "    count=0\n",
    "    with open(file_name, 'w') as rs:\n",
    "        for it in pcks:\n",
    "            times=[]\n",
    "            # 1) составляем временные промежутки для паков\n",
    "\n",
    "            with open('trans.csv', 'rb') as csvfile:\n",
    "                spamreader = csv.reader(csvfile, delimiter='|', quotechar='|')\n",
    "                spamreader.next()\n",
    "                for row in spamreader:\n",
    "                    if row[5]==it:\n",
    "                        times.append(int(row[2]))\n",
    "\n",
    "            difference=172800+43200 # = 2.5 days in seconds\n",
    "            # сравниваем даты по timestamp\n",
    "            # если покупки наблюдались в окрестности 2х дней, то\n",
    "            # будем считать, что в эти дни акция действовала\n",
    "            times.sort()\n",
    "\n",
    "            start_time=[]\n",
    "            end_time=[]\n",
    "\n",
    "            beg=times[0]\n",
    "\n",
    "            start_time.append(beg)\n",
    "            for item in times:\n",
    "                if beg+difference>=item:\n",
    "                    beg=item\n",
    "                else:\n",
    "                    start_time.append(item)\n",
    "                    end_time.append(beg)\n",
    "                    beg=item\n",
    "            end_time.append(beg)\n",
    "\n",
    "            # 2) проверяем временные промежутки; если все промежутки лежат в рамках 2.5 недель,\n",
    "            # то считаем пак акционным и проходимся априори алгоритмом по выявленным промежуткам\n",
    "\n",
    "            week_time=604800 # week time\n",
    "            access=True\n",
    "            for i in range(0,len(start_time)):\n",
    "                if end_time[i]-start_time[i] > week_time*2.5: # предполагаем, что акции длятся не более 2х с половиной недель\n",
    "                    access=False\n",
    "                    count=count+1\n",
    "\n",
    "            if access==True:\n",
    "                for bound in range(0,len(start_time)):\n",
    "                    ids={}\n",
    "                    # 3) если пак акционный, то забиваем данные для составления ассоциативных правил\n",
    "\n",
    "                    with open('trans.csv', 'rb') as csvfile:\n",
    "                        spam = csv.reader(csvfile, delimiter='|', quotechar='|')\n",
    "                        spam.next()\n",
    "                        for row in spam:\n",
    "                            if (int(row[2])>=start_time[bound] and int(row[2])<=end_time[bound]):\n",
    "                                if ids.has_key(int(row[0]))==True:\n",
    "                                    val=ids.get(int(row[0]))\n",
    "                                    if row[5] not in val:\n",
    "                                        val.append(row[5])\n",
    "                                else:\n",
    "                                    ids[int(row[0])]=[]# каждый id принимает в значение свои транзакции\n",
    "                                    ids[int(row[0])].append(row[5])\n",
    "                    \n",
    "                    with open('file_for_transactions.csv', 'w') as csf:\n",
    "                        for key, value in ids.items():\n",
    "                            st=\"\"\n",
    "                            for i in range(0,len(value)):\n",
    "                                st=st+value[i]+','\n",
    "                            if id_soc_data.has_key(key):\n",
    "                                if id_soc_data[key][0]!=\" \":    \n",
    "                                    st=st+id_soc_data[key][0]+\",\"\n",
    "                            csf.write(st+'\\n')\n",
    "                    #now1 = datetime.now()\n",
    "                    #print \"файлы: \",\"  \",(now1-now)\n",
    "\n",
    "                    inFile = dataFromFile(\"file_for_transactions.csv\")\n",
    "                    items, rules = runApriori(inFile, minSupport, minConfidence)    \n",
    "                    printResults(items, rules, rs, it)\n",
    "\n",
    "\n",
    "    # 4) время работы алгоритма\n",
    "    now1 = datetime.now()\n",
    "    print (now1-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:28.286215\n"
     ]
    }
   ],
   "source": [
    "write_res_to_file_name_with_additional_data(\"res_ad_data_l1.txt\", 0.2, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда рассматриваются все паки, результатом становятся достаточно очевидные вещи, как для US => fb, RU => vk/ok. Рассмотрим внутри одной соцсети - fb. \n",
    "Всего fb паков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "#social_netork='fb'\n",
    "social_network=\"fb\"\n",
    "pcks=[]\n",
    "with open('trans.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='|', quotechar='|')\n",
    "    pcks=[]\n",
    "    spamreader.next()\n",
    "    for row in spamreader:\n",
    "        if row[1]==social_network:\n",
    "            if (row[5] not in pcks):\n",
    "                pcks.append(row[5])\n",
    "    print len(pcks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ассоц. правила только по fb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def write_res_to_file_name_with_additional_data_concrete_soc(file_name, mnsp, mncf):\n",
    "    now = datetime.now()\n",
    "    minSupport=mnsp\n",
    "    minConfidence=mncf\n",
    "    \n",
    "    count=0\n",
    "    with open(file_name, 'w') as rs:\n",
    "        for it in pcks:\n",
    "            times=[]\n",
    "            # 1) составляем временные промежутки для паков\n",
    "\n",
    "            with open('trans.csv', 'rb') as csvfile:\n",
    "                spamreader = csv.reader(csvfile, delimiter='|', quotechar='|')\n",
    "                spamreader.next()\n",
    "                for row in spamreader:\n",
    "                    if row[5]==it:\n",
    "                        times.append(int(row[2]))\n",
    "\n",
    "            difference=172800+43200 # = 2.5 days in seconds\n",
    "            # сравниваем даты по timestamp\n",
    "            # если покупки наблюдались в окрестности 2х дней, то\n",
    "            # будем считать, что в эти дни акция действовала\n",
    "            times.sort()\n",
    "\n",
    "            start_time=[]\n",
    "            end_time=[]\n",
    "\n",
    "            beg=times[0]\n",
    "\n",
    "            start_time.append(beg)\n",
    "            for item in times:\n",
    "                if beg+difference>=item:\n",
    "                    beg=item\n",
    "                else:\n",
    "                    start_time.append(item)\n",
    "                    end_time.append(beg)\n",
    "                    beg=item\n",
    "            end_time.append(beg)\n",
    "\n",
    "            # 2) проверяем временные промежутки; если все промежутки лежат в рамках 2.5 недель,\n",
    "            # то считаем пак акционным и проходимся априори алгоритмом по выявленным промежуткам\n",
    "\n",
    "            week_time=604800 # week time\n",
    "            access=True\n",
    "            for i in range(0,len(start_time)):\n",
    "                if end_time[i]-start_time[i] > week_time*2.5: # предполагаем, что акции длятся не более 2х с половиной недель\n",
    "                    access=False\n",
    "                    count=count+1\n",
    "\n",
    "            if access==True:\n",
    "                for bound in range(0,len(start_time)):\n",
    "                    ids={}\n",
    "                    # 3) если пак акционный, то забиваем данные для составления ассоциативных правил\n",
    "\n",
    "                    with open('trans.csv', 'rb') as csvfile:\n",
    "                        spam = csv.reader(csvfile, delimiter='|', quotechar='|')\n",
    "                        spam.next()\n",
    "                        for row in spam:\n",
    "                            if row[1]==social_network:\n",
    "                                if (int(row[2])>=start_time[bound] and int(row[2])<=end_time[bound]):\n",
    "                                    if ids.has_key(int(row[0]))==True:\n",
    "                                        val=ids.get(int(row[0]))\n",
    "                                        if row[5] not in val:\n",
    "                                            val.append(row[5])\n",
    "                                    else:\n",
    "                                        ids[int(row[0])]=[]# каждый id принимает в значение свои транзакции\n",
    "                                        ids[int(row[0])].append(row[5])\n",
    "                    \n",
    "                    with open('file_for_transactions.csv', 'w') as csf:\n",
    "                        for key, value in ids.items():\n",
    "                            st=\"\"\n",
    "                            for i in range(0,len(value)):\n",
    "                                st=st+value[i]+','\n",
    "                            if id_soc_data.has_key(key):\n",
    "                                if id_soc_data[key][0]!=\" \":    \n",
    "                                    st=st+id_soc_data[key][0]+\",\"\n",
    "                            csf.write(st+'\\n')\n",
    "                    #now1 = datetime.now()\n",
    "                    #print \"файлы: \",\"  \",(now1-now)\n",
    "\n",
    "                    inFile = dataFromFile(\"file_for_transactions.csv\")\n",
    "                    items, rules = runApriori(inFile, minSupport, minConfidence)    \n",
    "                    printResults(items, rules, rs, it)\n",
    "\n",
    "\n",
    "    # 4) время работы алгоритма\n",
    "    now1 = datetime.now()\n",
    "    print (now1-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:06.599595\n"
     ]
    }
   ],
   "source": [
    "write_res_to_file_name_with_additional_data_concrete_soc(\"res_l1_fb.txt\", 0.2, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
